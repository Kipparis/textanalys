In [3]: pwd
Out[3]: '/home/kip/files/textanalys'

In [4]: path = '/home/kip/files/textanalys'

In [5]: import sys

In [6]: sys.path.insert(0, path)  

# Для улучшения классификатора  
? Если количество фичей гораздо больше количества комментариев.  
! Избегать переобучения выбирая Kernel functions (переобучение - модель хорошо работает на тренировочных данных, но на непривязанных очень плохо)
! Регуляризация важна
? Грамматические и орфографические ошибки в тексте  
!  стеммера Портера, а также библиотеку анализа морфологии pymorphy  
? Сохранение и загрузка модели
! joblib
+ svm с линейным ядром  
+ 1,2,3 - граммы как фичи, 2,3,4 буквенные граммы  
+ нормализовать текст, но попробовать оставить некоторые знаки препинания !?  
+ различные комбинации со включением стоп-слов, лемметизацией, стемметизацией, н-граммы из букв и слов, тонального словаря   
+ возможные классификаторы кроме SVM - Naive Bayes и MaxEnt.  
+ доп. признаки: морфологические теги (part-of-speech tag)
+ комбинация слов с тегами (напр: я-местоимения, люблю-глагол, чай-сущ.)  
+ **ещё какая - то фигнюшка с отнесением фичи к определённому семантическому классу**  


  
Создаём унограммы, биграммы как вектора признаков  
Используем дельта tf-idf как классификатор в перемешку с уже готовым словарём  

Создаём копию массива cnt только как словарь и заменяем там значения на дельту, в конце выводим  
  
?? признаки должны быть равны, но количество униграмм и биграмм в каждом тексте разное
  
Объединяя все слова из списка в один мы получим следующий отсортированный словарь (назовем его как базис вектор):  

[biography, cinema, feature, film, going, originally, part, remember, see]  

Заменяя предыдущие вектора на индекс слова в словаре получаем следующее:  
Проделав такую работу для всех отзывов мы можем получить достаточно большой список (в моем примере я взял 5000 самые часто встречающие слова). Эти вектора называются «векторами свойств» или же «features vector». Таким образом мы получаем вектора для каждого тестового отзыва, дальше мы сможем сравнивать эти вектора с помощью стандартных метрик, таких как Евклидовое расстояние, косинусное расстояние, и т.д. Данный подход называется «мешок слов» или же “Bag-Of-Words”.  
https://habr.com/post/263171/  
  
В альтернативном подходе мы попытаемся заменить каждое слово в списке номером его семантической группы. В итоге мы получим нечто вроде «мешка слов», но с более глубоким смыслом. Для этого используется технология Word2Vec от Google. Его можно найти в пакете библиотеки gensim, со встроенными моделями Word2Vec.   
  
Также можете составить тональный словарь, список слов с их значением тональности (affective lexicons). Либо вручную, либо перевести с других языков (напр. с английского) — вручную либо автоматически, либо одним из методов автоматического составления словаря тональности.   
+ Использование "мешка слов" подразумевает что n_features - некоторое кол-во уникальных слов в корпусе. Обычно это кол-во превышает 100к
  
-----------------  
Для того чтобы проанализировать текст, создаём вектор признаков "Мешок слов"  
Т.к. большиство из Х являются нулями, мешок слов является высоко размерным разреженным набором данных.  
  
Протестировать на точность стеминг и лемматизацию


  

Реализовать считывание фич из текстового файла для каждого игры->комментария

  
https://habr.com/post/149605/ -- основа -- "обучаем компьютер чувства"  
http://nlpx.net/archives/57 -- tf-idf  
http://www.long-short.pro/post/kross-validatsiya-cross-validation-304 -- кросс валидация  
https://scikit-learn.org/stable/index.html -- документация по используемым функциям  
https://toster.ru/q/21037 -- побольше инфы о том как реализовать модель
https://habr.com/post/264339/ -- помогло для изучения основы  
http://datascientist.one/support-vector-machines/ -- Support Vector Machines  
https://habr.com/company/ods/blog/326418/ -- Vowpal Wabbit для работы с текстом -- не использовалось  
https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/ -- сохранение и загрузка модели


Чем хороша эта метрика?  
1. Слова, неважные для вообще всех документов, например, предлоги или междометия — получат очень низкий вес TF-IDF (потому что часто встречаются во всех-всех документах), а важные — высокий.  
  
2. Её просто считать  

# TODO: 
+ Сделать вывод всех переменных модели и её вывода в один текстовый файл, добавляя туда дату для более быстрого поиска  
+ Создать класс settings.py откуда будут браться все коэффиценты:  
    - tf-idf коэффиценты для удаления  
    - кэффиценты модели для обучения  
+ Создать файл где у каждого слова будет delta td idf значения, для того чтобы внедрить pipe line для проверки вводимых данных  