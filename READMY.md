In [3]: pwd
Out[3]: '/home/kip/files/textanalys'

In [4]: path = '/home/kip/files/textanalys'

In [5]: import sys

In [6]: sys.path.insert(0, path)  
  
Создаём унограммы, биграммы как вектора признаков  
Используем дельта tf-idf как классификатор в перемешку с уже готовым словарём  

Создаём копию массива cnt только как словарь и заменяем там значения на дельту, в конце выводим  
  
?? признаки должны быть равны, но количество униграмм и биграмм в каждом тексте разное
  
Объединяя все слова из списка в один мы получим следующий отсортированный словарь (назовем его как базис вектор):  

[biography, cinema, feature, film, going, originally, part, remember, see]  

Заменяя предыдущие вектора на индекс слова в словаре получаем следующее:  
Проделав такую работу для всех отзывов мы можем получить достаточно большой список (в моем примере я взял 5000 самые часто встречающие слова). Эти вектора называются «векторами свойств» или же «features vector». Таким образом мы получаем вектора для каждого тестового отзыва, дальше мы сможем сравнивать эти вектора с помощью стандартных метрик, таких как Евклидовое расстояние, косинусное расстояние, и т.д. Данный подход называется «мешок слов» или же “Bag-Of-Words”.  
https://habr.com/post/263171/  
  
В альтернативном подходе мы попытаемся заменить каждое слово в списке номером его семантической группы. В итоге мы получим нечто вроде «мешка слов», но с более глубоким смыслом. Для этого используется технология Word2Vec от Google. Его можно найти в пакете библиотеки gensim, со встроенными моделями Word2Vec.   
  
Также можете составить тональный словарь, список слов с их значением тональности (affective lexicons). Либо вручную, либо перевести с других языков (напр. с английского) — вручную либо автоматически, либо одним из методов автоматического составления словаря тональности.   
+ Использование "мешка слов" подразумевает что n_features - некоторое кол-во уникальных слов в корпусе. Обычно это кол-во превышает 100к
  
-----------------  
Для того чтобы проанализировать текст, создаём вектор признаков "Мешок слов"  




  
https://habr.com/post/149605/ -- основа  
http://nlpx.net/archives/57 -- tf-idf  
Чем хороша эта метрика?  
1. Слова, неважные для вообще всех документов, например, предлоги или междометия — получат очень низкий вес TF-IDF (потому что часто встречаются во всех-всех документах), а важные — высокий.  
  
2. Её просто считать  